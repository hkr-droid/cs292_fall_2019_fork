\section{Methodology}
In this project we followed the paradigm presented in the first homework assignment which demonstrated Rosette as a solver-aided programming language.  To that end, we designed a DSL (domain-specific language) to simulate features of a concurrent programming language, using as a starting point the toy language Imp \cite{winskel1993}, which we have termed {\em ConImp}. Our main contributions to the language were adding the \texttt{par} and \texttt{atomic} operators which give us the semantic notion of concurrency, and an interpreter which offers a way to simulate a non-deterministic environment.  With these additions, we simulated a form of concurrency by being able to interleave commands at the will of a scheduler built into our interpreter.  We will elaborate on the design of our language by using the following code snippet as an example.  It is based on a midterm question used in the CS170 class.

\subsection{Imp as a DSL}
Rosette requires the user to provide a specification language that it can use to lift and convert ultimately into logical SMT formulas.  We chose Imp because it afforded the basic features necessary to test concurrent semantics, namely integer arithmetic and logical sequencing.  There are a number of examples and tutorials using custom DSLs  in the lectures of James Bornholt and Emina Torlak.  These however used fairly simple languages and could not ultimately provide the level of detail we needed to employ Rosette to any degree of utility with our own concurrent language.  We present in Listing 1 a basic example of the features of ConImp.  In the example we create two threads that each increment two global data values, with some of those increments being locked under a so-called "mutex".  The listing serves as a basis for our experimentations with the solver features afforded by Rosette as well.

\begin{table*}[!htbp]
\centering
\begin{tabular}{ r l r l }
\emph{Arithmetic Expression} & $e$ & $::=$ & $e + e$ 
                                         $|$ $e - e$ 
                                         $|$ $e * e$ 
                                         $|$ $v$ 
                                         $|$ $n$ \\
\emph{Boolean Expression} & $b$ & $::=$ & $\neg$ $b$ 
                                      $|$ $b \wedge b$ 
                                      $|$ $e = e$ 
                                      $|$ $e \leq e$ 
                                      $|$ true 
                                      $|$ false \\
\emph{Command} & $c$ & $::=$ & $v$ $:=$ $e$ 
							 $|$   \texttt{load} $g$ $v$
							 $|$   \texttt{store} $v$ $g$ \\ 
               &     &       $|$ & if $b$ then $c$ else $c$
                             $|$   while $b$ do $c$ \\
               &     &       $|$ & $c$ ; $c$
							 $|$   \texttt{atomic} $c$
							 $|$   \texttt{par} $c$
							 $|$   \texttt{skip}
							 $|$   \texttt{halt} 
                               \\
\emph{Local Variables} & $v$ & $\in$ & $\mathcal{V}$ \\
\emph{Global Variables} & $g$ & $\in$ & $\mathcal{G}$ \\
\emph{Integer} & $n$ & $\in$ & $\mathbb{Z}$ \\
\end{tabular}
\caption{
Concurrent Imp DSL grammar. 
}
\label{tab:dsl}
\end{table*}

\begin{lstlisting}[caption={Two threads concurrently updating a global store (ConImp).},captionpos=b,frame=single]
par A0 A1 with G = { A -> 5, B -> 50 }  
(define A0 (list
      (: (load 'A 'a)
      (: (:= 'a (add 'a 1))
      (: (store 'a 'A)
      (: (atomic
         (list (: (load 'A 'a)
               (: (:= 'a (add 'a 1))
               (: (store 'a 'A)
               (: (load 'B 'b)
               (: (:= 'b (add 'b 1))
                  (store 'b 'B))))))))
       (halt)))))))

(define A1 (list
      (: (atomic
         (list (: (load 'A 'a)
               (: (:= 'a (add 'a 1))
               (: (store 'a 'A)
               (: (load 'B 'b)
               (: (:= 'b (add 'b 1))
                  (store 'b 'B))))))))
      (halt))))
  
\end{lstlisting}

\subsubsection{Big-Step Semantics}
We first implemented Imp using a big-step style semantics. The big-step style is exemplified by having a simple recursive function evaluate all terms in a given expression until termination \cite{huttel2010}.  Most of the examples we found that defined a DSL to feed into Rosette were similarly given uncomplicated, big-step style semantics.  This can be straightforwardly accomplished by giving the terms of the language as ground functions of integer arity and then pattern matching over them.  The advantages of this approach are its simplicity and ease of setup.  There are disadvantages though in how the evaluation procedure is a monolithic function call.  That is, there was no way to inspect a single step of the evaluation, which is nececssary to do more complicated control-flow procedures.  In practice, any meaningful extension of a toy programming language should must evolve beyond a big-step style, or natural, semantics.

\subsubsection{Small-Step Semantics}
To remedy the above shortcomings, we realized we needed to adopt a more granular approach to evaluating expressions \cite{fernandez2004}.  The programming languages community defines an analogue to the big-step style, appropriately termed ``small-step semantics.''  In this protocol, the recursion is modded out and each pattern match, instead of \emph{destructing} a given term, \emph{contructs} a data structure representing the state of the overall evaluation.  A common notion of state used in the literature comes from the notion of an abstract machine \cite{huttel2010}.

Abstract machines are an approach to operational semantics that centralize the notion of registers, so that temporary evaluations of code, ad future directions of evaluation, are stored in registers as if they were stacks.   The tradition began with Peter Landin's SECD machine and was redesigned by Matthias Fellesen under the CESK moniker, an acronym that stands for (C)ontrol, (E)nvironment, (S)tore, (K)ontinutation.  We adopted the CESK approach to rebuilding our interpreter in the small-step style.

Intuitively, the C stack keeps the expressions currently being evaluated, the E and S stacks keep track of variable assignments, while the K stack maintains partially evaluated code that will be referenced later.  As an example, in the implementation of the while loop function, we can think of the entire while loop expression residing on the C stack.  It can be pattern matched over to split the command into two pieces, a boolean test and the set of possible commands to execute based on the test returning true or false.  We may put the boolean test on the C stack and the rest on the K stack, then proceed to evaluate the boolean test alone.  Based on the result of that test we may pop the K stack to the appropriate command, regarding the alternative option.  It is this interplay between the C and K stack, being able to store references to code with a placeholder value waiting on a future computation, that affords more complicated flow of control.  

\subsubsection{Par and Atomic}
After reimplementing our language as an abstract CESK machine, we had only really mimicked the functionality that we already had with the big-step semantics.  However, we could implement the \texttt{par} and \texttt{atomic} functions in the above style.

To some extent, we combined the interpreter and the semantics for the `par` function.  We designed it to randomly choose between a set of commands (implicit threads) and evaluate the command list a single step recursively.  After each step, the interpreter would be back to randomly choosing a command branch.  In this way we could interleave commands at a more granular level and simulate multiple branches of code running in parallel.

There must be a duality with the par function however, which is the user's ability to constrain parallel evaluation, an ability to tell the interpreter that a certain region of code may not be parallelized, or must be run sequentially, in an uninteruptible fashion.  We adopted the operator \texttt{atomic} for this purpose.  The user simply uses the \texttt{atomic} keyword followed by a Racket list of expressions.  If the interpreter sees the atomic keyword it circumvents the parallel choice and continues until the list of expressions have been evaluated.

There is a subtle distinction between providing the semantics for concurrent programming language and simulating its behavior.  We thus note the difference between the pattern matching rules we provide for our syntax and the interpreter which selects expressions of code to evaluate.  The interpreter implements a simple ``scheduler'' which in our case is a random choice among expressions.  By providing an interpreter ourselves, we gain the ability to run example code sequences.  Without such a function, we would have to manually construct a given ``run'' of the code.

\begin{figure}[!htbp]
%\begin{empheq}{align*}
\begin{align*}
\frac{\langle S_1 , s \rangle \Rightarrow \langle S_1' , s' \rangle}
	 {\langle S_1 \texttt{ par } S_2, s \rangle \Rightarrow \langle S_1' \texttt{ par } S_2, s' \rangle}
	 \textsc{Par}^1 
\\
\\
\frac{\langle S_1 , s \rangle \Rightarrow s' \rangle}
     {\langle S_1 \texttt{ par } S_2, s \rangle \Rightarrow \langle S_2, s' \rangle}  
	 \textsc{Par}^2
\end{align*}
%\end{empheq}
\caption{Semantics for the concurrent Imp DSL.
}
\label{fig:par-rules}
\end{figure}

\subsection{Rosette Integration}
Rosette takes expressions in the DSL annotated with Rosette constructs for symbolic variables.  The annotated code is then lifted into Rosette's internal language that affords functions for common tasks utilizing SMT solvers.  The three most common are solving, verification, and synthesis.  In the ``solve'' case, the user code has expressions replaced with symbolic variables, and is additionally given an assertion for what the code should do or compute down to. Rosette uses the backend SMT solver to find values for the symbolic variables to evaluate the expressions in a way that satisfies the assertion.  It can be thought of as a simplistic form of synthesis.  For the ``verify'' case, Rosette looks for a way to fill in the symbolic variables to find a counterexample to the user code's assertion. Finally, in the ``synthesis'' case, the user code is additionally given ``holes'' replacing whole expressions along with a specification for behavior.  Rosette employs its solver to find candidate expressions in the DSL for the holes that mantain the specification.  We found Rosette readily capable of solving and verifying expressions.  Unfortunately, we were not able to synthesize program holes with Rosette.

\subsection{The Midterm Example}
The example in Listing 1 above is based on a similar example written in C for the CS170 class.  The class example is designed to show that there is a data race for the integer assignment in thread 1 that is not locked by the mutex.  Technically, it is possible for thread 1 to being the integer assigment expression, then because it accesses data outside of a mutex, it could be pre-empted, or interrupted, by thread 2, which begins modifying the data inside its own locked region.  Thread 1, when it regains control of the cpu, will begin a logical line down from the initial line and so the integer assignment will be lost.  This suggests at least two possible pathways for the code.  In fact there are 3, with the third following the above logic but for the opposite interruption scheme.

There is a sense then in which Listing 1 is a subtly inexact rendering of Listing 2, stemming from the fact that there is no way for our interpreter to race the threads.  It is only capable of interleaving states.  The code in Listing 2 trades on a more complicated notion of execution where threads have a notion of ownership.  A given thread owns the CPU while it is executing, and by proxy, any global data.  Ownership can be preempted and in this case a thread will abandon a region of code that it was running.  That is to say, there is a wholly separate semantics for thread ownership, or data ownership that we have not programmed into our language.  The concept of data races such as this have been studied, for instance in the Rust programming language \cite{rust} where ownership is built into the type system itself.  It is not immediately clear how we could add this to our language without adopting types, nor how we would have our interpreter simulate the race without some sort of a clock signal.  This reinforces the distinction made earlier about implementing specific program behaviors in the object language or in the interpreter.  Nevertheless, both are interesting possible extensions to ConImp.  

\begin{lstlisting}[caption={Two threads concurrently updating a global store (C).},captionpos=b,frame=single]
#include <unistd.h>
#include <stdlib.h>
#include <stdio.h>
#include <pthread.h>

int A;
int B;

pthread_mutex_t Lock;

void *
Thread_1(void *arg)
{
	A++;
	pthread_mutex_lock(&Lock);
	A++;
	B++;
	pthread_mutex_unlock(&Lock);

	pthread_exit(NULL);
	return(NULL);
}

void *
Thread_2(void *arg)
{
	pthread_mutex_lock(&Lock);
	A++;
	B++;
	pthread_mutex_unlock(&Lock);

	pthread_exit(NULL);
	return(NULL);
}

int main(int argc, char *argv[])
{
	pthread_t t1;
	pthread_t t2;
	int err;

	pthread_mutex_init(&Lock,NULL);
	A = 5;
	B = 50;

	err = pthread_create(&t1, NULL, Thread_1, NULL);
	err = pthread_create(&t2, NULL, Thread_2, NULL);

	pthread_join(t1, NULL);
	pthread_join(t2, NULL);

	printf("A: %d, B: %d\n",A,B);

	pthread_exit(NULL);

	return(0);
}
\end{lstlisting}
